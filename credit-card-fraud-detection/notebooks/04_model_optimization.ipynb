{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 - Model Optimization\n",
                "\n",
                "Bu notebook'ta farklƒ± modelleri deneyip hyperparameter tuning yapacaƒüƒ±z.\n",
                "\n",
                "## Hedefler:\n",
                "- Farklƒ± modelleri kar≈üƒ±la≈ütƒ±rmak (Random Forest, XGBoost)\n",
                "- GridSearchCV ile hyperparameter tuning\n",
                "- En iyi modeli se√ßmek"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import xgboost as xgb\n",
                "import plotly.graph_objects as go\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "df = pd.read_csv('../data/creditcard.csv')\n",
                "\n",
                "X = df.drop('Class', axis=1)\n",
                "y = df['Class']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Apply SMOTE\n",
                "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"Training set: {X_train_smote.shape}\")\n",
                "print(f\"Test set: {X_test_scaled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
                "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
                "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
                "}\n",
                "\n",
                "# Train and evaluate each model\n",
                "results = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\nTraining {name}...\")\n",
                "    \n",
                "    # Train\n",
                "    model.fit(X_train_smote, y_train_smote)\n",
                "    \n",
                "    # Predict\n",
                "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
                "    \n",
                "    # Evaluate\n",
                "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "    \n",
                "    # Cross-validation\n",
                "    cv_scores = cross_val_score(\n",
                "        model, X_train_smote, y_train_smote,\n",
                "        cv=5, scoring='roc_auc', n_jobs=-1\n",
                "    )\n",
                "    \n",
                "    results[name] = {\n",
                "        'model': model,\n",
                "        'roc_auc': roc_auc,\n",
                "        'cv_mean': cv_scores.mean(),\n",
                "        'cv_std': cv_scores.std()\n",
                "    }\n",
                "    \n",
                "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
                "    print(f\"CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize model comparison\n",
                "model_names = list(results.keys())\n",
                "roc_aucs = [results[m]['roc_auc'] for m in model_names]\n",
                "cv_means = [results[m]['cv_mean'] for m in model_names]\n",
                "\n",
                "fig = go.Figure()\n",
                "\n",
                "fig.add_trace(go.Bar(\n",
                "    name='Test ROC-AUC',\n",
                "    x=model_names,\n",
                "    y=roc_aucs,\n",
                "    marker_color='lightblue'\n",
                "))\n",
                "\n",
                "fig.add_trace(go.Bar(\n",
                "    name='CV ROC-AUC',\n",
                "    x=model_names,\n",
                "    y=cv_means,\n",
                "    marker_color='darkblue'\n",
                "))\n",
                "\n",
                "fig.update_layout(\n",
                "    title='Model Performance Comparison',\n",
                "    yaxis_title='ROC-AUC Score',\n",
                "    barmode='group',\n",
                "    height=500\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hyperparameter Tuning - Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameter grid for Random Forest\n",
                "rf_param_grid = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [10, 20, None],\n",
                "    'min_samples_split': [5, 10],\n",
                "    'min_samples_leaf': [2, 4]\n",
                "}\n",
                "\n",
                "# GridSearchCV\n",
                "print(\"Starting GridSearchCV for Random Forest...\")\n",
                "rf_grid = GridSearchCV(\n",
                "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
                "    rf_param_grid,\n",
                "    cv=3,\n",
                "    scoring='roc_auc',\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "rf_grid.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
                "print(f\"Best CV score: {rf_grid.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning - XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameter grid for XGBoost\n",
                "xgb_param_grid = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [3, 6, 9],\n",
                "    'learning_rate': [0.01, 0.1],\n",
                "    'subsample': [0.8, 1.0]\n",
                "}\n",
                "\n",
                "# GridSearchCV\n",
                "print(\"Starting GridSearchCV for XGBoost...\")\n",
                "xgb_grid = GridSearchCV(\n",
                "    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
                "    xgb_param_grid,\n",
                "    cv=3,\n",
                "    scoring='roc_auc',\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "xgb_grid.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "print(f\"\\nBest parameters: {xgb_grid.best_params_}\")\n",
                "print(f\"Best CV score: {xgb_grid.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Select Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate tuned models\n",
                "tuned_models = {\n",
                "    'Random Forest (Tuned)': rf_grid.best_estimator_,\n",
                "    'XGBoost (Tuned)': xgb_grid.best_estimator_\n",
                "}\n",
                "\n",
                "tuned_results = {}\n",
                "\n",
                "for name, model in tuned_models.items():\n",
                "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
                "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "    \n",
                "    tuned_results[name] = roc_auc\n",
                "    print(f\"{name} - ROC-AUC: {roc_auc:.4f}\")\n",
                "\n",
                "# Select best model\n",
                "best_model_name = max(tuned_results, key=tuned_results.get)\n",
                "best_model = tuned_models[best_model_name]\n",
                "\n",
                "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
                "print(f\"ROC-AUC: {tuned_results[best_model_name]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary\n",
                "\n",
                "### Model Optimization Results:\n",
                "- Compared multiple models (Logistic Regression, Random Forest, XGBoost)\n",
                "- Performed hyperparameter tuning with GridSearchCV\n",
                "- Selected the best performing model\n",
                "\n",
                "### Next Steps:\n",
                "1. Detailed evaluation of the best model\n",
                "2. Feature importance analysis\n",
                "3. Final model validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"‚úÖ Model optimization completed!\")\n",
                "print(\"\\nNext: Run 05_model_evaluation.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}