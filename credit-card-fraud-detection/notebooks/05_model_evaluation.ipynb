{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05 - Model Evaluation\n",
                "\n",
                "Bu notebook'ta en iyi modeli detaylÄ± olarak deÄŸerlendireceÄŸiz.\n",
                "\n",
                "## Hedefler:\n",
                "- KapsamlÄ± model deÄŸerlendirmesi\n",
                "- Feature importance analizi\n",
                "- FarklÄ± threshold deÄŸerlerini test etme\n",
                "- Final model metrikleri"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    classification_report,\n",
                "    confusion_matrix,\n",
                "    roc_auc_score,\n",
                "    roc_curve,\n",
                "    precision_recall_curve,\n",
                "    average_precision_score,\n",
                "    f1_score\n",
                ")\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"âœ… Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Prepare Data and Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "df = pd.read_csv('../data/creditcard.csv')\n",
                "\n",
                "X = df.drop('Class', axis=1)\n",
                "y = df['Class']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Scale\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# SMOTE\n",
                "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
                "\n",
                "# Train best model (using Random Forest as example)\n",
                "final_model = RandomForestClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=20,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "print(\"Training final model...\")\n",
                "final_model.fit(X_train_smote, y_train_smote)\n",
                "print(\"âœ… Model trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Predictions and Basic Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predictions\n",
                "y_pred = final_model.predict(X_test_scaled)\n",
                "y_pred_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
                "\n",
                "# Metrics\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
                "\n",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
                "\n",
                "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
                "print(f\"Average Precision Score: {avg_precision:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "fig = go.Figure(data=go.Heatmap(\n",
                "    z=cm,\n",
                "    x=['Predicted Normal', 'Predicted Fraud'],\n",
                "    y=['Actual Normal', 'Actual Fraud'],\n",
                "    text=cm,\n",
                "    texttemplate='%{text}',\n",
                "    colorscale='Blues',\n",
                "    showscale=True\n",
                "))\n",
                "\n",
                "fig.update_layout(\n",
                "    title='Confusion Matrix',\n",
                "    height=500\n",
                ")\n",
                "fig.show()\n",
                "\n",
                "# Calculate metrics from confusion matrix\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f\"\\nTrue Negatives: {tn}\")\n",
                "print(f\"False Positives: {fp}\")\n",
                "print(f\"False Negatives: {fn}\")\n",
                "print(f\"True Positives: {tp}\")\n",
                "print(f\"\\nFalse Positive Rate: {fp/(fp+tn):.4f}\")\n",
                "print(f\"False Negative Rate: {fn/(fn+tp):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ROC and PR Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curve\n",
                "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
                "\n",
                "# Precision-Recall Curve\n",
                "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
                "\n",
                "# Create subplots\n",
                "fig = make_subplots(\n",
                "    rows=1, cols=2,\n",
                "    subplot_titles=('ROC Curve', 'Precision-Recall Curve')\n",
                ")\n",
                "\n",
                "# ROC Curve\n",
                "fig.add_trace(\n",
                "    go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={roc_auc:.4f})', line=dict(color='blue')),\n",
                "    row=1, col=1\n",
                ")\n",
                "fig.add_trace(\n",
                "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', line=dict(color='red', dash='dash')),\n",
                "    row=1, col=1\n",
                ")\n",
                "\n",
                "# PR Curve\n",
                "fig.add_trace(\n",
                "    go.Scatter(x=recall, y=precision, mode='lines', name=f'PR (AP={avg_precision:.4f})', line=dict(color='green')),\n",
                "    row=1, col=2\n",
                ")\n",
                "\n",
                "fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
                "fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
                "fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
                "fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
                "\n",
                "fig.update_layout(height=500, showlegend=True)\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': final_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"Top 15 Most Important Features:\")\n",
                "print(feature_importance.head(15))\n",
                "\n",
                "# Visualize\n",
                "fig = go.Figure()\n",
                "\n",
                "fig.add_trace(go.Bar(\n",
                "    x=feature_importance['importance'][:20],\n",
                "    y=feature_importance['feature'][:20],\n",
                "    orientation='h',\n",
                "    marker_color='steelblue'\n",
                "))\n",
                "\n",
                "fig.update_layout(\n",
                "    title='Top 20 Feature Importances',\n",
                "    xaxis_title='Importance',\n",
                "    yaxis_title='Features',\n",
                "    height=600,\n",
                "    yaxis={'categoryorder': 'total ascending'}\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Threshold Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different thresholds\n",
                "thresholds = np.arange(0.1, 1.0, 0.1)\n",
                "threshold_results = []\n",
                "\n",
                "for threshold in thresholds:\n",
                "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
                "    \n",
                "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
                "    tn, fp, fn, tp = cm.ravel()\n",
                "    \n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    threshold_results.append({\n",
                "        'threshold': threshold,\n",
                "        'precision': precision,\n",
                "        'recall': recall,\n",
                "        'f1': f1,\n",
                "        'tp': tp,\n",
                "        'fp': fp,\n",
                "        'fn': fn\n",
                "    })\n",
                "\n",
                "threshold_df = pd.DataFrame(threshold_results)\n",
                "print(threshold_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize threshold impact\n",
                "fig = go.Figure()\n",
                "\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=threshold_df['threshold'],\n",
                "    y=threshold_df['precision'],\n",
                "    mode='lines+markers',\n",
                "    name='Precision',\n",
                "    line=dict(color='blue')\n",
                "))\n",
                "\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=threshold_df['threshold'],\n",
                "    y=threshold_df['recall'],\n",
                "    mode='lines+markers',\n",
                "    name='Recall',\n",
                "    line=dict(color='red')\n",
                "))\n",
                "\n",
                "fig.add_trace(go.Scatter(\n",
                "    x=threshold_df['threshold'],\n",
                "    y=threshold_df['f1'],\n",
                "    mode='lines+markers',\n",
                "    name='F1-Score',\n",
                "    line=dict(color='green')\n",
                "))\n",
                "\n",
                "fig.update_layout(\n",
                "    title='Metrics vs Threshold',\n",
                "    xaxis_title='Threshold',\n",
                "    yaxis_title='Score',\n",
                "    height=500\n",
                ")\n",
                "fig.show()\n",
                "\n",
                "# Find optimal threshold (max F1)\n",
                "optimal_idx = threshold_df['f1'].idxmax()\n",
                "optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
                "print(f\"\\nğŸ¯ Optimal Threshold (max F1): {optimal_threshold:.2f}\")\n",
                "print(f\"F1-Score at optimal threshold: {threshold_df.loc[optimal_idx, 'f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Final Model Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary\n",
                "summary = f\"\"\"\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "           FINAL MODEL EVALUATION SUMMARY\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "Model: Random Forest Classifier\n",
                "\n",
                "Performance Metrics:\n",
                "  â€¢ ROC-AUC Score:          {roc_auc:.4f}\n",
                "  â€¢ Average Precision:      {avg_precision:.4f}\n",
                "  â€¢ Optimal Threshold:      {optimal_threshold:.2f}\n",
                "\n",
                "Confusion Matrix (at 0.5 threshold):\n",
                "  â€¢ True Negatives:         {tn}\n",
                "  â€¢ False Positives:        {fp}\n",
                "  â€¢ False Negatives:        {fn}\n",
                "  â€¢ True Positives:         {tp}\n",
                "\n",
                "Top 3 Important Features:\n",
                "  1. {feature_importance.iloc[0]['feature']}: {feature_importance.iloc[0]['importance']:.4f}\n",
                "  2. {feature_importance.iloc[1]['feature']}: {feature_importance.iloc[1]['importance']:.4f}\n",
                "  3. {feature_importance.iloc[2]['feature']}: {feature_importance.iloc[2]['importance']:.4f}\n",
                "\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\"\"\"\n",
                "\n",
                "print(summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary\n",
                "\n",
                "### Model Evaluation Complete:\n",
                "- Comprehensive performance metrics calculated\n",
                "- Feature importance analyzed\n",
                "- Optimal threshold identified\n",
                "- Model ready for deployment\n",
                "\n",
                "### Next Steps:\n",
                "1. Create final pipeline\n",
                "2. Save model for deployment\n",
                "3. Build deployment application"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"âœ… Model evaluation completed!\")\n",
                "print(\"\\nNext: Run 06_final_pipeline.ipynb to create the final pipeline\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}